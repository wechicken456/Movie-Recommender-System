{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "\n",
    "Let $\\mathbf{R} \\in \\mathbb{R}^{m \\times n}$ denote the interaction matrix with $m$ users and $n$ items, and the testues of $\\mathbf{R}$ represent explicit ratings. The user-item interaction will be factorized into a user latent matrix $\\mathbf{P} \\in \\mathbb{R}^{m \\times k}$ and an item latent matrix $\\mathbf{Q} \\in \\mathbb{R}^{n \\times k}$, where $k \\ll m, n$, is the latent factor size. \n",
    "\n",
    "Let $\\mathbf{p}_u$ denote the $u^{th}$ row of $\\mathbf{P}$ and $\\mathbf{q}_i$ denote the $i^{th}$ row of $\\mathbf{Q}$. For a given item $i$, the elements of $\\mathbf{q}_i$ measure the extent to which the item possesses those characteristics such as the genres and languages of a movie. For a given user $u$, the elements of $\\mathbf{p}_u$ measure the extent of interest the user has in items' corresponding characteristics. These latent factors might measure obvious dimensions as mentioned in those examples or are completely uninterpretable. The predicted ratings can be estimated by\n",
    "$$ \\hat{\\mathbf{R}} = \\mathbf{P} \\mathbf{Q}^{\\top} $$\n",
    "where $\\hat{\\mathbf{R}} \\in \\mathbb{R}^{m \\times n}$ is the predicted rating matrix which has the same shape as $\\mathbf{R}$. One major problem of this prediction rule is that users/items biases can not be modeled. For example, some users tend to give higher ratings or some items always get lower ratings due to poorer quality. These biases are commonplace in real-world applications. To capture these biases, user specific and item specific bias terms are introduced. Specifically, the predicted rating user $u$ gives to item $i$ is calculated by\n",
    "$$ \\hat{R}_{ui} = \\mathbf{p}_u \\mathbf{q}_i^{\\top} + b_u + b_i $$\n",
    "Then, we train the matrix factorization model by minimizing the mean squared error between predicted rating scores and real rating scores. The objective function is defined as follows:\n",
    "$$ \\underset{\\mathbf{P}, \\mathbf{Q}, b}{\\text{argmin}} \\sum_{(u,i) \\in \\mathcal{K}} \\| R_{ui} - \\hat{R}_{ui} \\|^2 + \\lambda (\\|\\mathbf{P}\\|_F^2 + \\|\\mathbf{Q}\\|_F^2 + b_u^2 + b_i^2) $$\n",
    "where $\\lambda$ denotes the regularization rate. The regularizing term $\\lambda (\\|\\mathbf{P}\\|_F^2 + \\|\\mathbf{Q}\\|_F^2 + b_u^2 + b_i^2)$ is used to avoid over-fitting by penalizing the magnitude of the parameters. The $(u,i)$ pairs for which $R_{ui}$ is known are stored in the set $\\mathcal{K} = \\{(u,i) \\mid R_{ui} \\text{ is known}\\}$. The model parameters can be learned with an optimization algorithm, such as Stochastic Gradient Descent and Adam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected devices: []\n"
     ]
    }
   ],
   "source": [
    "from d2l import torch as d2l\n",
    "import os\n",
    "import pandas as pd\n",
    "from d2l import torch as d2l\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import data_DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import itertools\n",
    "\n",
    "from common import MemoryMonitor\n",
    "from dataset import TorchSharedTensorDataset\n",
    "\n",
    "\n",
    "devices = d2l.try_all_gpus()\n",
    "print(f\"Detected devices: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(d2l.Module):\n",
    "    def __init__(self, num_latent, num_users, num_movies, **kwargs):\n",
    "        super(MF, self).__init__(**kwargs)\n",
    "        self.P = nn.Embedding(num_users, num_latent)\n",
    "        self.Q = nn.Embedding(num_movies, num_latent)\n",
    "        self.num_latent = num_latent\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.movie_bias = nn.Embedding(num_movies, 1)\n",
    "\n",
    "    def forward(self, user_id, movie_id):\n",
    "        \"\"\"\n",
    "        user_id and movie_id should be tensors of shape (batch_size,).\n",
    "        This function computes a pairwise dot product between each user and movie pair in the batch.\n",
    "        Returns a tensor of shape (batch_size, 1) containing the predicted ratings for each pair.\n",
    "        \"\"\"\n",
    "        if user_id.shape != movie_id.shape:\n",
    "            raise \"user_id and movie_id must have the same shape.\"  \n",
    "    \n",
    "        # if this is a single scalar tensor, add a dimension to make it a batch of size 1\n",
    "        if user_id.ndim == 0: \n",
    "            user_id = user_id.unsqueeze(0)\n",
    "        if movie_id.ndim == 0:\n",
    "            movie_id = movie_id.unsqueeze(0)\n",
    "\n",
    "        # convert user_id and movie_id to long tensors if they are not already\n",
    "        if user_id.dtype != torch.long:\n",
    "            user_id = user_id.long()\n",
    "        if movie_id.dtype != torch.long:\n",
    "            movie_id = movie_id.long()\n",
    "\n",
    "        # ensure user_id and movie_id are on the same device as the model parameters\n",
    "        user_id = user_id.to(self.P.weight.device)\n",
    "        movie_id = movie_id.to(self.Q.weight.device)\n",
    "        \n",
    "        P_u = self.P(user_id)\n",
    "        Q_m = self.Q(movie_id)\n",
    "        user_bias = self.user_bias(user_id)\n",
    "        movie_bias = self.movie_bias(movie_id)\n",
    "        outputs = torch.sum(P_u * Q_m, dim = 1, keepdim=True) + user_bias + movie_bias\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator - difference between predicted and real rating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(net, test_iter, device=None):\n",
    "    \"\"\"\n",
    "    Compute the RMSE of the model `net` on the test set `test_iter`\n",
    "    `test_iter` generally consists of batches of (batch_size, 1, 1, 1) tuples. Each tuple is (user_id, movie_id, rating), where `rating` is the ground truth rating.\n",
    "    \"\"\"\n",
    "    mse = nn.MSELoss() # torch doesn't have a built-in RMSE loss, so we use MSE and compute RMSE from it\n",
    "    rmse = lambda y_hat, y: torch.sqrt(mse(y_hat, y))\n",
    "    rmse_list = []\n",
    "\n",
    "    for batch in test_iter:\n",
    "        if type(batch) is tuple:\n",
    "            users, movies, ratings = batch\n",
    "        else:\n",
    "            users, movies, ratings = batch[:, 0], batch[:, 1], batch[:, 2]\n",
    "        outputs = net(users, movies).squeeze(1)\n",
    "        rmse_list.append(rmse(outputs, ratings.to(device)))\n",
    "    rmse_list = torch.tensor(rmse_list, device=device)\n",
    "    return rmse_list.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Etestuating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerDDP(d2l.Trainer):\n",
    "    def __init__(self, max_epochs, optimizer, loss, ddp_rank, world_size, lr = 0.002, wd = 1e-5, gradient_clip_test=0, save_every_n_epochs=2):\n",
    "        self.save_hyperparameters()\n",
    "        super().__init__(max_epochs)\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.optim = optimizer\n",
    "        self.loss = loss\n",
    "        self.world_size = world_size\n",
    "        self.ddp_rank = ddp_rank\n",
    "        self.save_every_n_epochs = save_every_n_epochs\n",
    "        # To plot the training and test losses\n",
    "        self.board = d2l.ProgressBoard()\n",
    "\n",
    "\n",
    "    def prepare_model(self, model):\n",
    "        \"\"\"\n",
    "        The model should already be wrapped around DDP() on the correct device (self.device).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "            \n",
    "\n",
    "    def prepare_data(self, train_iter, test_iter):\n",
    "        self.train_dataloader = train_iter\n",
    "        self.test_dataloader = test_iter\n",
    "        self.num_train_batches = len(self.train_dataloader)\n",
    "        self.num_test_batches = (len(self.test_dataloader)\n",
    "                                if self.test_dataloader is not None else 0)\n",
    "\n",
    "    def fit(self, model, train_iter, test_iter):\n",
    "\n",
    "        self.prepare_data(train_iter, test_iter)\n",
    "        self.prepare_model(model)\n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.test_batch_idx = 0\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()    \n",
    "            \n",
    "            # save model checkpoint\n",
    "            if self.ddp_rank == 0 and (self.epoch  % self.save_every_n_epochs == 0 or self.epoch == self.max_epochs - 1):\n",
    "                checkpoint_path = \"./mf_checkpoint.pth\"\n",
    "                torch.save(self.model.state_dict(), checkpoint_path)\n",
    "                print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "\n",
    "    def fit_epoch(self):\n",
    "        self.model.train()\n",
    "        dist.barrier()\n",
    "        self.train_dataloader.sampler.set_epoch(self.epoch)  # set seed for shuffling in DDP\n",
    "\n",
    "        total_loss = torch.tensor(0.0, device=self.ddp_rank)\n",
    "        batch_size = 0\n",
    "        print(f\"Rank {self.ddp_rank}, Epoch {self.epoch + 1}/{self.max_epochs}, Dataloader size: {len(self.train_dataloader)}. START.\", flush = True)\n",
    "        for batch in self.train_dataloader:\n",
    "            batch_size = len(batch)\n",
    "            loss = self.training_step(self.prepare_batch(batch))\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                total_loss += loss\n",
    "\n",
    "            self.train_batch_idx += 1\n",
    "        print(f\"Rank {self.ddp_rank}, Epoch {self.epoch + 1}/{self.max_epochs}, Dataloader size: {len(self.train_dataloader)}. DONE.\", flush = True)\n",
    "        \n",
    "        # Average the loss across all batches\n",
    "        total_loss = total_loss / self.train_batch_idx  \n",
    "        local_train_loss = total_loss.clone()  # Keep a local copy of the loss for logging\n",
    "\n",
    "        # Synchronize and average the total loss across all processes\n",
    "        dist.all_reduce(total_loss, op=torch.distributed.ReduceOp.SUM) \n",
    "        total_loss = total_loss / self.world_size  \n",
    "        self.train_losses.append(total_loss)\n",
    "\n",
    "        if self.test_dataloader is None:\n",
    "            return\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = self.test_step()\n",
    "            local_test_loss = loss.clone()  # Keep a local copy of the loss for logging\n",
    "            dist.all_reduce(loss, op=torch.distributed.ReduceOp.SUM) # Add up the losses from all processes\n",
    "            self.test_losses.append(loss / self.world_size)  # Average the loss across all processes\n",
    "            self.test_batch_idx += 1\n",
    "\n",
    "        if self.ddp_rank == 0:       \n",
    "            self.plot('loss', self.train_losses[-1], train=True)\n",
    "            self.plot('loss', self.test_losses[-1], train=False)\n",
    "            plt.savefig(\"mf_loss.png\")\n",
    "        \n",
    "        print(f\"Rank {self.ddp_rank}, Epoch {self.epoch + 1}/{self.max_epochs}. \\nAvg Train Loss: {self.train_losses[-1]:.4f}, Avg Test RMSE: {self.test_losses[-1]:.4f}, Local Train Loss: {local_train_loss:.4f}, Local Test Loss: {local_test_loss:.4f}.\\nTrain Batch: {self.train_batch_idx}, Test Batch: {self.test_batch_idx}, # of samples in train batch: {batch_size}\", flush = True)\n",
    "\n",
    "    \n",
    "    def prepare_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Prepare the batch for training or testing.\n",
    "        This method is called implicitly by fit() before each batch is passed to the forward call of self.model\n",
    "        Note that due to the nature of memory-mapped file in distributed dataloading (`dataset.py`), \n",
    "        the dataloader will use tensors instead of TensorDataset. \n",
    "        Hence, each batch is a tensor of [[u1, m1, r1], [u2, m2, r2]...,] where each [u, m, r] is a tensor of (user_id, movie_id, rating).\n",
    "        So we need to convert the batch to [users, movies, ratings] before passing it to the model.\n",
    "        batch is a tensor of [[u1, m1, r1], [u2, m2, r2]...,] where each [u, m, r] is a tensor of (user_id, movie_id, rating).\n",
    "        \"\"\"\n",
    "        users, movies, ratings = batch[:, 0], batch[:, 1], batch[:, 2]\n",
    "        return users, movies, ratings\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        users, movies, ratings = batch\n",
    "        outputs = self.model(users, movies).squeeze(1)\n",
    "        loss = self.loss(outputs, ratings.to(self.ddp_rank))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self):\n",
    "        loss = evaluator(self.model, self.test_dataloader, self.ddp_rank)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def plot(self, key, value, train):\n",
    "        \"\"\"Plot a point in animation.\"\"\"\n",
    "        self.board.xlabel = 'epoch'\n",
    "        self.board.draw(self.epoch, d2l.numpy(d2l.to(value, d2l.cpu())),\n",
    "                        ('train_' if train else 'val_') + key,\n",
    "                        every_n=self.save_every_n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddp_setup():\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12555\"\n",
    "    os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"  \n",
    "\n",
    "def worker(rank, world_size):    \n",
    "    torch.cuda.set_device(rank)\n",
    "    print(f\"Process {rank} started. Device: {torch.cuda.current_device()}. World size: {world_size}.\", flush=True)\n",
    "    ddp_setup()\n",
    "\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    batch_size = 64\n",
    "    \n",
    "    monitor = MemoryMonitor()\n",
    "\n",
    "    print(f\"Process {rank} akdalkfslakflaksdlfklak\", flush=True)\n",
    "\n",
    "    # Only rank 0 will load the datasets and create the dataloaders.\n",
    "    if rank == 0:\n",
    "        num_users, num_movies, train_set, test_set = data_DDP.get_datasets(test_ratio=0.1, batch_size=batch_size, world_size=world_size, is_big_computer=False)\n",
    "        train_set = TorchSharedTensorDataset(data=train_set, is_rank0=True, world_size=world_size, metadata={'num_users': num_users, 'num_movies': num_movies})\n",
    "        test_set = TorchSharedTensorDataset(data=test_set, is_rank0=True, world_size =world_size, metadata={'num_users': num_users, 'num_movies': num_movies})\n",
    "    else:\n",
    "        train_set = TorchSharedTensorDataset(data=None, is_rank0=False, world_size=world_size)\n",
    "        test_set = TorchSharedTensorDataset(data=None, is_rank0=False, world_size=world_size)\n",
    "\n",
    "    if rank == 0:\n",
    "        print(monitor.table())\n",
    "\n",
    "    dist.barrier()\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size, drop_last=True,\n",
    "                                            shuffle=False, sampler=DistributedSampler(train_set, rank=rank, num_replicas = world_size, shuffle = True, drop_last=True)) \n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size, drop_last=True,\n",
    "                                            shuffle=False, sampler=DistributedSampler(test_set, rank=rank, num_replicas = world_size, shuffle = False, drop_last=True))\n",
    "\n",
    "   \n",
    "    \n",
    "    # Setup memory monitor\n",
    "    pids = [os.getpid()]\n",
    "    all_pids = [None for _ in range(world_size)]\n",
    "    dist.all_gather_object(all_pids, pids)\n",
    "    all_pids = list(itertools.chain.from_iterable(all_pids))\n",
    "    monitor = MemoryMonitor(all_pids)\n",
    "    if rank == 0:\n",
    "        print(monitor.table())\n",
    "\n",
    "    num_users = train_set.metadata['num_users']\n",
    "    num_movies = train_set.metadata['num_movies']\n",
    "    print(f\"Rank: {rank}, finished loading data. Number of users: {num_users}. Number of movies: {num_movies}. Train set size: {len(train_set)}. Test set size: {len(test_set)}\", flush=True)\n",
    "\n",
    "\n",
    "    # Create the model and wrap it in DDP\n",
    "    net = MF(32, num_users, num_movies).to(rank)\n",
    "    ddp_net = DDP(net, device_ids=[rank])\n",
    "\n",
    "\n",
    "    lr = 0.002  \n",
    "    wd = 1e-5\n",
    "    num_epochs = 20\n",
    "    optimizer = torch.optim.Adam(ddp_net.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    trainer = TrainerDDP(max_epochs=num_epochs, optimizer=optimizer, loss=loss_fn, lr=lr, \n",
    "                        wd=wd, ddp_rank = rank, world_size = world_size, save_every_n_epochs=1)\n",
    "\n",
    "    trainer.fit(ddp_net, train_iter, test_iter)\n",
    "\n",
    "\n",
    "    if rank == 0:\n",
    "        CHECKPOINT_PATH = \"./mf_checkpoint.pth\"\n",
    "        torch.save(ddp_net.state_dict(), CHECKPOINT_PATH)\n",
    "\n",
    "    dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m world_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m num_users, num_movies, train_set, test_set \n\u001b[0;32m----> 4\u001b[0m num_users, num_movies, train_set, test_set \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_big_computer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished loading data. Number of users:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_users, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of movies:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_movies)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworld_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Code/programming/DeepLearning/Movie_Recommender/data_DDP.py:172\u001b[0m, in \u001b[0;36mget_datasets\u001b[0;34m(split_mode, feedback, test_ratio, is_big_computer, batch_size)\u001b[0m\n\u001b[1;32m    170\u001b[0m world_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()  \n\u001b[1;32m    171\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_set)           \n\u001b[0;32m--> 172\u001b[0m samples_to_keep \u001b[38;5;241m=\u001b[39m (\u001b[43mtotal_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m (world_size \u001b[38;5;241m*\u001b[39m batch_size)\n\u001b[1;32m    173\u001b[0m trimmed_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(samples_to_keep))\n\u001b[1;32m    174\u001b[0m train_set \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mSubset(train_set, trimmed_indices)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    world_size = torch.cuda.device_count()\n",
    "    print(f\"Using {world_size} for training.\")\n",
    "    mp.spawn(worker, args=(world_size,), nprocs=world_size, join=True)    \n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Movie_Rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
