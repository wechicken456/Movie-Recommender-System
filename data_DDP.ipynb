{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import os\n",
    "import pandas as pd\n",
    "from d2l import torch as d2l\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from common import MemoryMonitor\n",
    "from dataset import TorchSharedTensorDataset\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group, barrier, all_reduce\n",
    "import torch.multiprocessing as mp\n",
    "import itertools\n",
    "\n",
    "import logging \n",
    "\n",
    "\n",
    "devices = d2l.try_all_gpus()\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(is_big_computer = False):\n",
    "    names = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "    data = pd.read_csv(\"./ml-32m/ratings.csv\", names=names, header=0)\n",
    "    if not is_big_computer:\n",
    "        data = data.sample(n=10_000 + (10_000 % 32), random_state=42) # Limit to 1 million ratings for faster processing\n",
    "    else:\n",
    "        data = data.sample(n=1_000_000 + (1_000_000 % 512), random_state=42) # Limit to 1 million ratings for faster processing\n",
    "\n",
    "    data[\"user_id\"] = data[\"user_id\"]\n",
    "    data[\"movie_id\"] = data[\"movie_id\"]\n",
    "    num_users = data.user_id.unique().shape[0]\n",
    "    num_movies = data.movie_id.unique().shape[0]\n",
    "    return data, num_users, num_movies\n",
    "\n",
    "def reindex_data(data : pd.DataFrame):\n",
    "    user_id_map = {id: i for i, id in enumerate(data.user_id.unique())}\n",
    "    movie_id_map = {id: i for i, id in enumerate(data.movie_id.unique())}\n",
    "    \n",
    "    data['user_id'] = data['user_id'].map(user_id_map)\n",
    "    data['movie_id'] = data['movie_id'].map(movie_id_map)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, num_users, num_movies = read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity = 1 - len(data) / (num_users * num_movies)\n",
    "# print(f'number of users: {num_users}, number of movies: {num_movies}')\n",
    "# print(f'matrix sparsity: {sparsity:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the distribution of the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(data[\"rating\"], bins=5, edgecolor='black')\n",
    "# plt.xlabel('Rating')\n",
    "# plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset\n",
    "We split the dataset into training and test sets. The following function provides two split modes including `random` and `seq-aware`. In the `random` mode, the function splits the 100k interactions randomly without considering timestamp and uses the 90% of the data as training samples and the rest 10% as test samples by default. In the `seq-aware` mode, we leave out the movie that a user rated most recently for test, and usersâ€™ historical interactions as training set. User historical interactions are sorted from oldest to newest based on timestamp. This mode will be used in the sequence-aware recommendation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data : pd.DataFrame, num_users, num_movies, split_mode = \"random\", test_ratio=0.2):\n",
    "    if split_mode == \"random\":\n",
    "        training_mask = np.random.rand(len(data)) > test_ratio\n",
    "        train_data = data[training_mask]\n",
    "        test_data = data[~training_mask]\n",
    "    \n",
    "    elif split_mode == \"seq-aware\":\n",
    "        user_groups = data.groupby('user_id')\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        for _, group in user_groups:\n",
    "            group = group.sort_values(by='timestamp', ascending=True)\n",
    "            split_index = int(len(group) * (1 - test_ratio))\n",
    "            train_data.append(group.iloc[:split_index])\n",
    "            test_data.append(group.iloc[split_index:])\n",
    "        train_data = pd.DataFrame(pd.concat(train_data, ignore_index=True))\n",
    "        test_data = pd.DataFrame(pd.concat(test_data, ignore_index=True))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"split_mode must be 'random' or 'seq-aware'\")\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data : pd.DataFrame, num_users, num_movies, feedback=\"explicit\"):\n",
    "    \"\"\"\n",
    "    returns lists of users, movies, ratings and a dictionary/matrix that records the interactions. \n",
    "    If feedback is \"explicit\", ratings are used as feedback.\n",
    "    If feedback is \"implicit\", then the user didn't give any rating, so the user's action of interacting with the movie is considered as positive feedback.\n",
    "    The `inter` is the interaction matrix that reflects this.\n",
    "    \"\"\"\n",
    "    inter = np.zeros((num_movies, num_users)) if feedback == 'explicit' else {}\n",
    "\n",
    "    if feedback == \"explicit\":\n",
    "        scores = data[\"rating\"].astype(int)\n",
    "    else:\n",
    "        scores = pd.Series(1, index=data.index)\n",
    "    \n",
    "    i = 0\n",
    "    for line in data.itertuples(): # itertuples is faster than iterrows for large DataFrames\n",
    "        user_id, movie_id = int(line.user_id), int(line.movie_id)\n",
    "        if feedback == \"explicit\":\n",
    "            inter[movie_id - 1, user_id - 1] = scores.iloc[i]\n",
    "        else:\n",
    "            inter.setdefault(user_id - 1, []).append(movie_id - 1)\n",
    "        i += 1\n",
    "        \n",
    "    return list(data[\"user_id\"]), list(data[\"movie_id\"]), list(scores), inter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(split_mode = \"random\", feedback=\"explicit\", test_ratio = 0.1, is_big_computer = False, batch_size=64, world_size = 0):\n",
    "    data, num_users, num_movies = read_dataset(is_big_computer)\n",
    "    data = reindex_data(data)\n",
    "    train_data, test_data = split_data(data, num_users, num_movies, split_mode=split_mode, test_ratio=test_ratio)\n",
    "\n",
    "    train_users, train_movies, train_scores, train_inter = load_data(train_data, num_users, num_movies, feedback=feedback)\n",
    "    test_users, test_movies, test_scores, test_inter = load_data(test_data, num_users, num_movies, feedback=feedback)\n",
    "\n",
    "    train_u = torch.tensor(np.array(train_users))\n",
    "    train_m = torch.tensor(np.array(train_movies))\n",
    "    train_r = torch.tensor(np.array(train_scores), dtype=torch.float32)\n",
    "    test_u = torch.tensor(np.array(test_users))\n",
    "    test_m = torch.tensor(np.array(test_movies))\n",
    "    test_r = torch.tensor(np.array(test_scores), dtype=torch.float32)\n",
    "\n",
    "    train_set = torch.stack([train_u, train_m, train_r], dim=1)\n",
    "    test_set = torch.stack([test_u, test_m, test_r], dim=1) \n",
    "    # train_set = torch.utils.data.TensorDataset(train_u, train_m, train_r)\n",
    "    # test_set = torch.utils.data.TensorDataset(test_u, test_m, test_r)\n",
    "\n",
    "    #  Round the training set down to nearest multiple of world_size * batch_size\n",
    "    if world_size == 0:\n",
    "        world_size = 1\n",
    "    total_samples = len(train_set)           \n",
    "    samples_to_keep = (total_samples // (world_size * batch_size)) * (world_size * batch_size)\n",
    "    trimmed_indices = list(range(samples_to_keep))\n",
    "    # train_set = torch.utils.data.Subset(train_set, trimmed_indices)\n",
    "    train_set = train_set[trimmed_indices]\n",
    "    print(f\"TRAIN SET: total_samples: {total_samples}, samples_to_keep: {samples_to_keep}, world_size: {world_size}, batch_size: {batch_size}\")\n",
    "\n",
    "    # Round the test set down to nearest multiple of world_size * batch_size\n",
    "    total_samples = len(test_set)\n",
    "    samples_to_keep = (total_samples // (world_size * batch_size)) * (world_size * batch_size)\n",
    "    trimmed_indices = list(range(samples_to_keep))\n",
    "    # test_set = torch.utils.data.Subset(test_set, trimmed_indices)\n",
    "    test_set = test_set[trimmed_indices]\n",
    "    print(f\"TEST SET: total_samples: {total_samples}, samples_to_keep: {samples_to_keep}, world_size: {world_size}, batch_size: {batch_size}\")\n",
    "\n",
    "\n",
    "    return num_users, num_movies, train_set, test_set\n",
    "\n",
    "\n",
    "def get_dataloaders(train_set : torch.utils.data.TensorDataset, test_set: torch.utils.data.TensorDataset, batch_size = 64):\n",
    "    # Don't shuffle as we're using the Distributed Sampler here.\n",
    "    # The trainer should call train_iter.sampler.set_epoch(epoch) at every epoch to shuffle.\n",
    "    # Each process will receive batch_size samples per iteration.\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size=64, drop_last=True,\n",
    "                                            shuffle=False, sampler=DistributedSampler(train_set, shuffle = True, drop_last=True)) \n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size=64, drop_last=True,\n",
    "                                            shuffle=False, sampler=DistributedSampler(test_set, shuffle = False, drop_last=True))\n",
    "    return train_iter, test_iter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_users, num_movies, train_set, test_set = get_datasets(test_ratio=0.1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddp_setup():\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    #os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"  \n",
    "\n",
    "def worker(rank, world_size):    \n",
    "    ddp_setup()\n",
    "\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    # dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "    print(f\"local_rank={rank}, world_size={world_size}\")\n",
    "\n",
    "    monitor = MemoryMonitor()\n",
    "\n",
    "    # Only rank 0 will load the datasets and create the dataloaders.\n",
    "    if rank == 0:\n",
    "        num_users, num_movies, train_set, test_set = get_datasets(test_ratio=0.1, batch_size=64, world_size=world_size, is_big_computer=False)\n",
    "        train_set = TorchSharedTensorDataset(data=train_set, is_rank0=True, world_size=world_size, metadata={'num_users': num_users, 'num_movies': num_movies})\n",
    "        test_set = TorchSharedTensorDataset(data=test_set, is_rank0=True, world_size =world_size, metadata={'num_users': num_users, 'num_movies': num_movies})\n",
    "    else:\n",
    "        train_set = TorchSharedTensorDataset(data=None, is_rank0=False, world_size=world_size)\n",
    "        test_set = TorchSharedTensorDataset(data=None, is_rank0=False, world_size=world_size)\n",
    "\n",
    "    print(monitor.table())\n",
    "    barrier()\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size=64, drop_last=True,\n",
    "                                            shuffle=False, sampler=DistributedSampler(train_set, num_replicas = world_size, shuffle = True, drop_last=True)) \n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size=64, drop_last=True,\n",
    "                                            shuffle=False, sampler=DistributedSampler(test_set, num_replicas = world_size, shuffle = False, drop_last=True))\n",
    "\n",
    "    pids = [os.getpid()]\n",
    "    all_pids = [None for _ in range(world_size)]\n",
    "    dist.all_gather_object(all_pids, pids)\n",
    "    all_pids = list(itertools.chain.from_iterable(all_pids))\n",
    "    monitor = MemoryMonitor(all_pids)\n",
    "\n",
    "    for _ in range(100):\n",
    "        for batch in train_iter:\n",
    "            if rank == 0:\n",
    "                print(monitor.table())\n",
    "            \n",
    "            dist.barrier()\n",
    "            logger.warning(f'rank: {rank}, batch[0] = {batch[0]}')  # just make sure the data is correct\n",
    "            dist.barrier()\n",
    "\n",
    "    dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # world_size = torch.cuda.device_count()\n",
    "    world_size = 4 # For testing purposes, we can set it to 1 or 2\n",
    "    global num_users, num_movies, train_set, test_set \n",
    "    print(f\"Using {world_size} for training.\")\n",
    "    mp.spawn(worker, args=(world_size,), nprocs=world_size, join=True)    \n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_users, num_movies, train_set, test_set = get_datasets(test_ratio=0.1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Movie_Rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
